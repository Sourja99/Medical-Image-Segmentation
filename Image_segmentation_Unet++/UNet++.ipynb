{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!fusermount -u drive\n",
        "!google-drive-ocamlfuse drive"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u_3o7NQS3UWE",
        "outputId": "62455d92-7b61-4c1f-d734-e2468f29c20b"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shell-init: error retrieving current directory: getcwd: cannot access parent directories: Transport endpoint is not connected\n",
            "fusermount: bad mount point drive: Transport endpoint is not connected\n",
            "shell-init: error retrieving current directory: getcwd: cannot access parent directories: Transport endpoint is not connected\n",
            "/bin/bash: google-drive-ocamlfuse: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60F2hkca0DQR",
        "outputId": "c65f2546-9daa-434d-deb8-2b7cbe68c0af"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd drive/MyDrive/Image_segmentation_Unet++"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uPsz07L80WZc",
        "outputId": "aa0a0557-3f36-4c75-f3de-1c2d3790ca4a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Image_segmentation_Unet++\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TII9jRox0xyH",
        "outputId": "b7d07582-8a0f-452e-ca37-060647635edc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "config.yaml  \u001b[0m\u001b[01;34moutput\u001b[0m/     \u001b[01;34m__pycache__\u001b[0m/  requirements.txt  train.py\n",
            "\u001b[01;34minput\u001b[0m/       predict.py  README.md     \u001b[01;34msource\u001b[0m/           UNet++.ipynb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "BBMgYKg8q7Gx"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import yaml\n",
        "import torch\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from glob import glob\n",
        "from tqdm import tqdm\n",
        "from collections import OrderedDict\n",
        "from albumentations.augmentations import transforms\n",
        "from sklearn.model_selection import train_test_split\n",
        "from albumentations.core.composition import Compose, OneOf\n",
        "\n",
        "# from train import train, validate\n",
        "# from source.network import UNetPP\n",
        "# from source.dataset import DataSet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "tveCh-UQq7Gy"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch.utils.data\n",
        "\n",
        "\n",
        "class DataSet(torch.utils.data.Dataset):\n",
        "    def __init__(self, img_ids, img_dir, mask_dir, img_ext, mask_ext, transform=None):\n",
        "        self.img_ids = img_ids\n",
        "        self.img_dir = img_dir\n",
        "        self.mask_dir = mask_dir\n",
        "        self.img_ext = img_ext\n",
        "        self.mask_ext = mask_ext\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_id = self.img_ids[idx]\n",
        "\n",
        "        img = cv2.imread(os.path.join(self.img_dir, img_id + self.img_ext))\n",
        "\n",
        "        mask = []\n",
        "        mask.append(cv2.imread(os.path.join(self.mask_dir,\n",
        "                                            img_id + self.mask_ext), cv2.IMREAD_GRAYSCALE)[..., None])\n",
        "        mask = np.dstack(mask)\n",
        "\n",
        "        if self.transform is not None:\n",
        "            augmented = self.transform(image=img, mask=mask)\n",
        "            img = augmented['image']\n",
        "            mask = augmented['mask']\n",
        "\n",
        "        img = img.astype('float32') / 255\n",
        "        img = img.transpose(2, 0, 1)\n",
        "        mask = mask.astype('float32') / 255\n",
        "        mask = mask.transpose(2, 0, 1)\n",
        "\n",
        "        return img, mask, {'img_id': img_id}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "FE0FbiZnq7Gz"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "class VGGBlock(nn.Module):\n",
        "    def __init__(self, in_channels, middle_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv1 = nn.Conv2d(in_channels, middle_channels, 3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(middle_channels)\n",
        "        self.conv2 = nn.Conv2d(middle_channels, out_channels, 3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class UNetPP(nn.Module):\n",
        "    def __init__(self, num_classes, input_channels=3, deep_supervision=False, **kwargs):\n",
        "        super().__init__()\n",
        "\n",
        "        nb_filter = [32, 64, 128, 256, 512]\n",
        "\n",
        "        self.deep_supervision = deep_supervision\n",
        "\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "\n",
        "        self.conv0_0 = VGGBlock(input_channels, nb_filter[0], nb_filter[0])\n",
        "        self.conv1_0 = VGGBlock(nb_filter[0], nb_filter[1], nb_filter[1])\n",
        "        self.conv2_0 = VGGBlock(nb_filter[1], nb_filter[2], nb_filter[2])\n",
        "        self.conv3_0 = VGGBlock(nb_filter[2], nb_filter[3], nb_filter[3])\n",
        "        self.conv4_0 = VGGBlock(nb_filter[3], nb_filter[4], nb_filter[4])\n",
        "\n",
        "        self.conv0_1 = VGGBlock(nb_filter[0]+nb_filter[1], nb_filter[0], nb_filter[0])\n",
        "        self.conv1_1 = VGGBlock(nb_filter[1]+nb_filter[2], nb_filter[1], nb_filter[1])\n",
        "        self.conv2_1 = VGGBlock(nb_filter[2]+nb_filter[3], nb_filter[2], nb_filter[2])\n",
        "        self.conv3_1 = VGGBlock(nb_filter[3]+nb_filter[4], nb_filter[3], nb_filter[3])\n",
        "\n",
        "        self.conv0_2 = VGGBlock(nb_filter[0]*2+nb_filter[1], nb_filter[0], nb_filter[0])\n",
        "        self.conv1_2 = VGGBlock(nb_filter[1]*2+nb_filter[2], nb_filter[1], nb_filter[1])\n",
        "        self.conv2_2 = VGGBlock(nb_filter[2]*2+nb_filter[3], nb_filter[2], nb_filter[2])\n",
        "\n",
        "        self.conv0_3 = VGGBlock(nb_filter[0]*3+nb_filter[1], nb_filter[0], nb_filter[0])\n",
        "        self.conv1_3 = VGGBlock(nb_filter[1]*3+nb_filter[2], nb_filter[1], nb_filter[1])\n",
        "\n",
        "        self.conv0_4 = VGGBlock(nb_filter[0]*4+nb_filter[1], nb_filter[0], nb_filter[0])\n",
        "\n",
        "        if self.deep_supervision:\n",
        "            self.final1 = nn.Conv2d(nb_filter[0], num_classes, kernel_size=1)\n",
        "            self.final2 = nn.Conv2d(nb_filter[0], num_classes, kernel_size=1)\n",
        "            self.final3 = nn.Conv2d(nb_filter[0], num_classes, kernel_size=1)\n",
        "            self.final4 = nn.Conv2d(nb_filter[0], num_classes, kernel_size=1)\n",
        "        else:\n",
        "            self.final = nn.Conv2d(nb_filter[0], num_classes, kernel_size=1)\n",
        "\n",
        "\n",
        "    def forward(self, input):\n",
        "        x0_0 = self.conv0_0(input)\n",
        "        x1_0 = self.conv1_0(self.pool(x0_0))\n",
        "        x0_1 = self.conv0_1(torch.cat([x0_0, self.up(x1_0)], 1))\n",
        "\n",
        "        x2_0 = self.conv2_0(self.pool(x1_0))\n",
        "        x1_1 = self.conv1_1(torch.cat([x1_0, self.up(x2_0)], 1))\n",
        "        x0_2 = self.conv0_2(torch.cat([x0_0, x0_1, self.up(x1_1)], 1))\n",
        "\n",
        "        x3_0 = self.conv3_0(self.pool(x2_0))\n",
        "        x2_1 = self.conv2_1(torch.cat([x2_0, self.up(x3_0)], 1))\n",
        "        x1_2 = self.conv1_2(torch.cat([x1_0, x1_1, self.up(x2_1)], 1))\n",
        "        x0_3 = self.conv0_3(torch.cat([x0_0, x0_1, x0_2, self.up(x1_2)], 1))\n",
        "\n",
        "        x4_0 = self.conv4_0(self.pool(x3_0))\n",
        "        x3_1 = self.conv3_1(torch.cat([x3_0, self.up(x4_0)], 1))\n",
        "        x2_2 = self.conv2_2(torch.cat([x2_0, x2_1, self.up(x3_1)], 1))\n",
        "        x1_3 = self.conv1_3(torch.cat([x1_0, x1_1, x1_2, self.up(x2_2)], 1))\n",
        "        x0_4 = self.conv0_4(torch.cat([x0_0, x0_1, x0_2, x0_3, self.up(x1_3)], 1))\n",
        "\n",
        "        if self.deep_supervision:\n",
        "            output1 = self.final1(x0_1)\n",
        "            output2 = self.final2(x0_2)\n",
        "            output3 = self.final3(x0_3)\n",
        "            output4 = self.final4(x0_4)\n",
        "            return [output1, output2, output3, output4]\n",
        "\n",
        "        else:\n",
        "            output = self.final(x0_4)\n",
        "            return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "3XwFutz6q7G1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "\n",
        "def iou_score(output, target):\n",
        "    smooth = 1e-5\n",
        "\n",
        "    if torch.is_tensor(output):\n",
        "        output = torch.sigmoid(output).data.cpu().numpy()\n",
        "    if torch.is_tensor(target):\n",
        "        target = target.data.cpu().numpy()\n",
        "    output_ = output > 0.5\n",
        "    target_ = target > 0.5\n",
        "    intersection = (output_ & target_).sum()\n",
        "    union = (output_ | target_).sum()\n",
        "\n",
        "    return (intersection + smooth) / (union + smooth)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "bnlPqXYqq7G1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import yaml\n",
        "import torch\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from glob import glob\n",
        "from tqdm import tqdm\n",
        "from collections import OrderedDict\n",
        "from source.utils import iou_score, AverageMeter\n",
        "from albumentations import Resize\n",
        "from albumentations.augmentations import transforms\n",
        "from sklearn.model_selection import train_test_split\n",
        "from albumentations.core.composition import Compose, OneOf\n",
        "from albumentations.augmentations.geometric.rotate import RandomRotate90\n",
        "from source.network import UNetPP\n",
        "from source.dataset import DataSet\n",
        "\n",
        "\n",
        "def train(deep_sup, train_loader, model, criterion, optimizer):\n",
        "    avg_meters = {'loss': AverageMeter(),\n",
        "                  'iou': AverageMeter()}\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    pbar = tqdm(total=len(train_loader))\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    for input, target, _ in train_loader:\n",
        "        input = input.to(device)\n",
        "        target = target.to(device)\n",
        "\n",
        "        # compute output\n",
        "        if deep_sup:\n",
        "            outputs = model(input)\n",
        "            loss = 0\n",
        "            for output in outputs:\n",
        "                loss += criterion(output, target)\n",
        "            loss /= len(outputs)\n",
        "            iou = iou_score(outputs[-1], target)\n",
        "        else:\n",
        "            output = model(input)\n",
        "            loss = criterion(output, target)\n",
        "            iou = iou_score(output, target)\n",
        "\n",
        "        # compute gradient and do optimizing step\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        avg_meters['loss'].update(loss.item(), input.size(0))\n",
        "        avg_meters['iou'].update(iou, input.size(0))\n",
        "\n",
        "        postfix = OrderedDict([\n",
        "            ('loss', avg_meters['loss'].avg),\n",
        "            ('iou', avg_meters['iou'].avg),\n",
        "        ])\n",
        "        pbar.set_postfix(postfix)\n",
        "        pbar.update(1)\n",
        "    pbar.close()\n",
        "\n",
        "    return OrderedDict([('loss', avg_meters['loss'].avg),\n",
        "                        ('iou', avg_meters['iou'].avg)])\n",
        "\n",
        "\n",
        "def validate(deep_sup, val_loader, model, criterion):\n",
        "    avg_meters = {'loss': AverageMeter(),\n",
        "                  'iou': AverageMeter()}\n",
        "\n",
        "    # switch to evaluate mode\n",
        "    model.eval()\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        pbar = tqdm(total=len(val_loader))\n",
        "        for input, target, _ in val_loader:\n",
        "            input = input.to(device)\n",
        "            target = target.to(device)\n",
        "\n",
        "            # compute output\n",
        "            if deep_sup:\n",
        "                outputs = model(input)\n",
        "                loss = 0\n",
        "                for output in outputs:\n",
        "                    loss += criterion(output, target)\n",
        "                loss /= len(outputs)\n",
        "                iou = iou_score(outputs[-1], target)\n",
        "            else:\n",
        "                output = model(input)\n",
        "                loss = criterion(output, target)\n",
        "                iou = iou_score(output, target)\n",
        "\n",
        "            avg_meters['loss'].update(loss.item(), input.size(0))\n",
        "            avg_meters['iou'].update(iou, input.size(0))\n",
        "\n",
        "            postfix = OrderedDict([\n",
        "                ('loss', avg_meters['loss'].avg),\n",
        "                ('iou', avg_meters['iou'].avg),\n",
        "            ])\n",
        "            pbar.set_postfix(postfix)\n",
        "            pbar.update(1)\n",
        "        pbar.close()\n",
        "\n",
        "    return OrderedDict([('loss', avg_meters['loss'].avg),\n",
        "                        ('iou', avg_meters['iou'].avg)])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y7egjXf3z_gv"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "sEy2V84Nq7G3"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import yaml\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from source.network import UNetPP\n",
        "from argparse import ArgumentParser\n",
        "from albumentations.augmentations import transforms\n",
        "from albumentations.augmentations.geometric import resize \n",
        "from albumentations.core.composition import Compose\n",
        "\n",
        "\n",
        "val_transform = Compose([\n",
        "    resize.Resize(256, 256),\n",
        "    transforms.Normalize(),\n",
        "])\n",
        "\n",
        "\n",
        "def image_loader(image_name):\n",
        "    img = cv2.imread(image_name)\n",
        "    img = val_transform(image=img)[\"image\"]\n",
        "    img = img.astype('float32') / 255\n",
        "    img = img.transpose(2, 0, 1)\n",
        "\n",
        "    return img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "vuK03g82q7G3"
      },
      "outputs": [],
      "source": [
        "with open(\"config.yaml\") as f:\n",
        "    config = yaml.safe_load(f)\n",
        "\n",
        "extn = config[\"extn\"]\n",
        "epochs = config[\"epochs\"]\n",
        "log_path = config[\"log_path\"]\n",
        "mask_path = config[\"mask_path\"]\n",
        "image_path = config[\"image_path\"]\n",
        "model_path = config[\"model_path\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNo_RbpCq7G4"
      },
      "source": [
        "## Create log file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "exNq7lwBq7G6"
      },
      "outputs": [],
      "source": [
        "log = OrderedDict([\n",
        "    ('epoch', []),\n",
        "    ('loss', []),\n",
        "    ('iou', []),\n",
        "    ('val_loss', []),\n",
        "    ('val_iou', []),\n",
        "])\n",
        "\n",
        "best_iou = 0\n",
        "trigger = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "On2cNxvfq7G6"
      },
      "source": [
        "## Split images into train and validation set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "7odkjgF7q7G7"
      },
      "outputs": [],
      "source": [
        "extn_ = f\"*{extn}\"\n",
        "img_ids = glob(os.path.join(image_path, extn_))\n",
        "img_ids = [os.path.splitext(os.path.basename(p))[0] for p in img_ids]\n",
        "train_img_ids, val_img_ids = train_test_split(img_ids, test_size=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from albumentations.augmentations.geometric import rotate"
      ],
      "metadata": {
        "id": "oNYZZY0W6GoD"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4upxvEfYq7G7"
      },
      "source": [
        "## Define data transformations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "eT-1Rw5vq7G7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0538f2b4-e4f9-452c-e83a-16bb014875d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/albumentations/augmentations/transforms.py:1615: FutureWarning: This class has been deprecated. Please use RandomBrightnessContrast\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/albumentations/augmentations/transforms.py:1641: FutureWarning: RandomContrast has been deprecated. Please use RandomBrightnessContrast\n",
            "  FutureWarning,\n"
          ]
        }
      ],
      "source": [
        "train_transform = Compose([\n",
        "    rotate.RandomRotate90(),\n",
        "    transforms.Flip(),\n",
        "    OneOf([\n",
        "        transforms.HueSaturationValue(),\n",
        "        transforms.RandomBrightness(),\n",
        "        transforms.RandomContrast(),\n",
        "    ], p=1),\n",
        "    resize.Resize(256, 256),\n",
        "    transforms.Normalize(),\n",
        "])\n",
        "\n",
        "val_transform = Compose([\n",
        "    resize.Resize(256, 256),\n",
        "    transforms.Normalize(),\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UiTFTvGq7G8"
      },
      "source": [
        "## Create train and validation dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Dl22PmgWq7G8"
      },
      "outputs": [],
      "source": [
        "train_dataset = DataSet(\n",
        "    img_ids=train_img_ids,\n",
        "    img_dir=image_path,\n",
        "    mask_dir=mask_path,\n",
        "    img_ext=extn,\n",
        "    mask_ext=extn,\n",
        "    transform=train_transform)\n",
        "\n",
        "val_dataset = DataSet(\n",
        "    img_ids=val_img_ids,\n",
        "    img_dir=image_path,\n",
        "    mask_dir=mask_path,\n",
        "    img_ext=extn,\n",
        "    mask_ext=extn,\n",
        "    transform=val_transform)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGS_zAMgq7G8"
      },
      "source": [
        "## Create train and validation data loaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "W65ezZeHq7G9"
      },
      "outputs": [],
      "source": [
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=16,\n",
        "    shuffle=True,\n",
        "    drop_last=True)\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=16,\n",
        "    shuffle=False,\n",
        "    drop_last=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pvhvmysvq7G9"
      },
      "source": [
        "## Create the model object"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "CmWPIvnIq7G9"
      },
      "outputs": [],
      "source": [
        "# Create model object\n",
        "model = UNetPP(1, 3, True)\n",
        "\n",
        "# Port model to GPU if it is available\n",
        "if torch.cuda.is_available():\n",
        "    model.cuda()\n",
        "    \n",
        "# Define Loss Function\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "params = filter(lambda p: p.requires_grad, model.parameters())\n",
        "\n",
        "# Define Optimizer\n",
        "optimizer = optim.Adam(params, lr=1e-3, weight_decay=1e-4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mHct2ukq7G-"
      },
      "source": [
        "## Run the train loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jDLmC1GNq7G-"
      },
      "outputs": [],
      "source": [
        "for epoch in range(epochs):\n",
        "    print(f'Epoch [{epoch}/{epochs}]')\n",
        "\n",
        "    # train for one epoch\n",
        "    train_log = train(True, train_loader, model, criterion, optimizer)\n",
        "    # evaluate on validation set\n",
        "    val_log = validate(True, val_loader, model, criterion)\n",
        "\n",
        "    print('loss %.4f - iou %.4f - val_loss %.4f - val_iou %.4f'\n",
        "              % (train_log['loss'], train_log['iou'], val_log['loss'], val_log['iou']))\n",
        "\n",
        "    log['epoch'].append(epoch)\n",
        "    log['loss'].append(train_log['loss'])\n",
        "    log['iou'].append(train_log['iou'])  \n",
        "    log['val_loss'].append(val_log['loss'])\n",
        "    log['val_iou'].append(val_log['iou'])\n",
        "\n",
        "    pd.DataFrame(log).to_csv(log_path, index=False)\n",
        "\n",
        "    trigger += 1\n",
        "\n",
        "    if val_log['iou'] > best_iou:\n",
        "        torch.save(model.state_dict(), model_path)\n",
        "        best_iou = val_log['iou']\n",
        "        print(\"=> saved best model\")\n",
        "        trigger = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "kOWexfJPq7G_"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import yaml\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from predict import image_loader\n",
        "from source.network import UNetPP\n",
        "from argparse import ArgumentParser\n",
        "from albumentations.augmentations import transforms\n",
        "from albumentations.core.composition import Compose\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PmjOMOKTq7G_"
      },
      "source": [
        "## Create validation transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "ex2m_Aczq7G_"
      },
      "outputs": [],
      "source": [
        "val_transform = Compose([\n",
        "    resize.Resize(256, 256),\n",
        "    transforms.Normalize(),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "PPE1VvYtq7G_"
      },
      "outputs": [],
      "source": [
        "with open(\"config.yaml\") as f:\n",
        "    config = yaml.safe_load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "EjN-zTsbq7G_"
      },
      "outputs": [],
      "source": [
        "im_width = config[\"im_width\"]\n",
        "im_height = config[\"im_height\"]\n",
        "model_path = config[\"model_path\"]\n",
        "output_path = config[\"output_path\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77Kl9TBrq7HA"
      },
      "source": [
        "## Load the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "kb0Kg9oPq7HA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa072008-11ab-4c4a-8c49-8a4b625dc9d2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "UNetPP(\n",
              "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (up): Upsample(scale_factor=2.0, mode=bilinear)\n",
              "  (conv0_0): VGGBlock(\n",
              "    (relu): ReLU(inplace=True)\n",
              "    (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (conv1_0): VGGBlock(\n",
              "    (relu): ReLU(inplace=True)\n",
              "    (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (conv2_0): VGGBlock(\n",
              "    (relu): ReLU(inplace=True)\n",
              "    (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (conv3_0): VGGBlock(\n",
              "    (relu): ReLU(inplace=True)\n",
              "    (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (conv4_0): VGGBlock(\n",
              "    (relu): ReLU(inplace=True)\n",
              "    (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (conv0_1): VGGBlock(\n",
              "    (relu): ReLU(inplace=True)\n",
              "    (conv1): Conv2d(96, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (conv1_1): VGGBlock(\n",
              "    (relu): ReLU(inplace=True)\n",
              "    (conv1): Conv2d(192, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (conv2_1): VGGBlock(\n",
              "    (relu): ReLU(inplace=True)\n",
              "    (conv1): Conv2d(384, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (conv3_1): VGGBlock(\n",
              "    (relu): ReLU(inplace=True)\n",
              "    (conv1): Conv2d(768, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (conv0_2): VGGBlock(\n",
              "    (relu): ReLU(inplace=True)\n",
              "    (conv1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (conv1_2): VGGBlock(\n",
              "    (relu): ReLU(inplace=True)\n",
              "    (conv1): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (conv2_2): VGGBlock(\n",
              "    (relu): ReLU(inplace=True)\n",
              "    (conv1): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (conv0_3): VGGBlock(\n",
              "    (relu): ReLU(inplace=True)\n",
              "    (conv1): Conv2d(160, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (conv1_3): VGGBlock(\n",
              "    (relu): ReLU(inplace=True)\n",
              "    (conv1): Conv2d(320, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (conv0_4): VGGBlock(\n",
              "    (relu): ReLU(inplace=True)\n",
              "    (conv1): Conv2d(192, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (final1): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n",
              "  (final2): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n",
              "  (final3): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n",
              "  (final4): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "# Create model object\n",
        "model = UNetPP(1, 3, True)\n",
        "\n",
        "# Load pre-trained weights\n",
        "model.load_state_dict(torch.load(model_path))\n",
        "\n",
        "# Port the model to GPU if it is available\n",
        "if torch.cuda.is_available():\n",
        "    model.cuda()\n",
        "    \n",
        "# Set model mode to evaluation\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SnXLcnQbq7HA"
      },
      "source": [
        "## Load the test image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "mOW36OI6q7HA"
      },
      "outputs": [],
      "source": [
        "test_img = \"input/PNG/Original/115.png\"\n",
        "image = image_loader(test_img)\n",
        "\n",
        "# Convert the image to a batch of 1 image\n",
        "image = np.expand_dims(image,0)\n",
        "\n",
        "# Convert numpy array to torch tensor\n",
        "image = torch.from_numpy(image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "oNg7v35Aq7HA"
      },
      "outputs": [],
      "source": [
        "# Port the image to GPU if it is available\n",
        "if torch.cuda.is_available():\n",
        "    image = image.to(device=\"cuda\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIWqwKYYq7HA"
      },
      "source": [
        "## Make prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "i9kJAR14q7HB"
      },
      "outputs": [],
      "source": [
        "mask = model(image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "IAwdqdeCq7HB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "outputId": "5076f930-aa49-4f00-fc06-cf0c60a3699a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f7a76483b50>"
            ]
          },
          "metadata": {},
          "execution_count": 27
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "mask = mask[-1]\n",
        "\n",
        "# Convert torch tensor to numpy array\n",
        "mask = mask.detach().cpu().numpy()\n",
        "\n",
        "# Convert output to a 2-d array\n",
        "mask = np.squeeze(np.squeeze(mask, axis=0), axis=0)\n",
        "\n",
        "# Convert output to binary based on threshold\n",
        "mask[mask > -2.5] = 255\n",
        "mask[mask <= -2.5] = 0\n",
        "\n",
        "# Resize the ouptut image to input image size\n",
        "mask = cv2.resize(mask, (im_width, im_height))\n",
        "\n",
        "# Plot the generated mask\n",
        "plt.imshow(mask, cmap=\"gray\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gMblvSNq7HB"
      },
      "source": [
        "## Read and plot the ground truth mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "S_XMr1Dsq7HB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "outputId": "6d94463b-07be-4748-ecc7-960c767a179b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f7a70085650>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "actual_mask = \"input/PNG/Ground Truth/115.png\"\n",
        "am = plt.imread(actual_mask)\n",
        "plt.imshow(am, cmap=\"gray\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d7i7cFl8q7HB"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}